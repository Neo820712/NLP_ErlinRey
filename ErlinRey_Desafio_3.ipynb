{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g3yeJGnCYxuF"
      },
      "source": [
        "<img src=\"https://github.com/hernancontigiani/ceia_memorias_especializacion/raw/master/Figures/logoFIUBA.jpg\" width=\"500\" align=\"center\">\n",
        "\n",
        "\n",
        "# Procesamiento de lenguaje natural\n",
        "## Modelo de lenguaje con tokenización por caracteres"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iv5PEwGzZA9-"
      },
      "source": [
        "### Consigna\n",
        "- Seleccionar un corpus de texto sobre el cual entrenar el modelo de lenguaje.\n",
        "- Realizar el pre-procesamiento adecuado para tokenizar el corpus, estructurar el dataset y separar entre datos de entrenamiento y validación.\n",
        "- Proponer arquitecturas de redes neuronales basadas en unidades recurrentes para implementar un modelo de lenguaje.\n",
        "- Con el o los modelos que consideren adecuados, generar nuevas secuencias a partir de secuencias de contexto con las estrategias de greedy search y beam search determístico y estocástico. En este último caso observar el efecto de la temperatura en la generación de secuencias.\n",
        "\n",
        "\n",
        "### Sugerencias\n",
        "- Durante el entrenamiento, guiarse por el descenso de la perplejidad en los datos de validación para finalizar el entrenamiento. Para ello se provee un callback.\n",
        "- Explorar utilizar SimpleRNN (celda de Elman), LSTM y GRU.\n",
        "- rmsprop es el optimizador recomendado para la buena convergencia. No obstante se pueden explorar otros.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Y-QdFbHZYj7C"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import io\n",
        "import pickle\n",
        "\n",
        "import os\n",
        "import platform\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
        "from scipy.special import softmax\n",
        "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
        "\n",
        "import urllib.request\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xTvXlEKQZdqx"
      },
      "source": [
        "### Datos\n",
        "Se utilizará el libro Don Quijote en Project Gutenberg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "7amy6uUaBLVD",
        "outputId": "7ffe9f22-70a4-4800-8150-5cf36d1f9795",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7c083a924b30>"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "# Fijamos la semilla para reproducibilidad\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "6v_ickFwBJTy",
        "outputId": "5e56a541-c57c-4507-fe10-4ec844349650",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "El archivo quijote.txt ya existe.\n",
            "\n",
            "Longitud total del corpus: 2110737 caracteres\n"
          ]
        }
      ],
      "source": [
        "# URL del libro en formato texto plano (UTF-8)\n",
        "# ID 2000 es Don Quijote en Project Gutenberg\n",
        "url = 'https://www.gutenberg.org/ebooks/2000.txt.utf-8'\n",
        "filename = 'quijote.txt'\n",
        "\n",
        "#Descargar el archivo si no existe\n",
        "if not os.path.exists(filename):\n",
        "    print(f\"Descargando {filename}...\")\n",
        "    urllib.request.urlretrieve(url, filename)\n",
        "    print(\"Descarga completada.\")\n",
        "else:\n",
        "    print(f\"El archivo {filename} ya existe.\")\n",
        "\n",
        "# Leer el texto\n",
        "with open(filename, 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "# Limpieza de cabeceras y pies de página de Gutenberg\n",
        "start_marker = \"*** START OF THE PROJECT GUTENBERG EBOOK\"\n",
        "end_marker = \"*** END OF THE PROJECT GUTENBERG EBOOK\"\n",
        "\n",
        "start_idx = text.find(start_marker)\n",
        "end_idx = text.find(end_marker)\n",
        "\n",
        "\n",
        "if start_idx != -1 and end_idx != -1:\n",
        "    start_idx = text.find(\"\\n\", start_idx) + 1\n",
        "    text = text[start_idx:end_idx]\n",
        "\n",
        "# Normalización\n",
        "article_text = text.lower()\n",
        "\n",
        "print(f\"\\nLongitud total del corpus: {len(article_text)} caracteres\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "WBE0sSYuB-E6",
        "outputId": "13c1ef35-d51b-4755-8532-57714d747f3e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Longitud del texto: 2110737\n",
            "--- Fragmento del inicio ---\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "el ingenioso hidalgo don quijote de la mancha\n",
            "\n",
            "\n",
            "\n",
            "por miguel de cervantes saavedra\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "el ingenioso hidalgo don quijote de la mancha\n",
            "\n",
            "\n",
            "  \n",
            "tasa\n",
            "\n",
            "  \n",
            "testimonio de las erratas\n",
            "\n",
            "  \n",
            "el rey\n",
            "\n",
            "  \n",
            "al duque de béjar\n",
            "\n",
            "  \n",
            "prólogo\n",
            "\n",
            "  \n",
            "al libro de don quijote de la mancha\n",
            "\n",
            "\n",
            "\n",
            "que trata de la condición y ejercicio del famoso\n",
            "hidalgo don quijote de la mancha\n",
            "\n",
            "que trata de la primera salida que de su tierra hizo\n",
            "el ingenioso don quijote\n",
            "\n",
            "donde se cuenta la graciosa manera que tuvo don\n",
            "quijote en armarse cabal\n",
            "\n",
            "--- Fragmento aleatorio del medio ---\n",
            "halle desapercebido el enemigo; pero si se tomara mi\n",
            "consejo, aconsejárale yo que usara de una prevención, de la cual su\n",
            "majestad la hora de agora debe estar muy ajeno de pensar en ella.\n",
            "\n",
            "apenas oyó esto el cura, cuando dijo entre sí:\n",
            "\n",
            "— ¡dios te tenga de su mano, pobre don quijote: que me parece que te\n",
            "despeñas de la alta cumbre de tu locura hasta el profundo abismo de tu\n",
            "simplicidad!\n",
            "\n",
            "mas el barbero, que ya había dado en el mesmo pensamiento que el cura,\n",
            "preguntó a don quijote cuál era la adve\n"
          ]
        }
      ],
      "source": [
        "# Verificamos la longitud y mostramos fragmentos para confirmar la limpieza\n",
        "print(f\"Longitud del texto: {len(article_text)}\")\n",
        "\n",
        "print(\"--- Fragmento del inicio ---\")\n",
        "print(article_text[:500])\n",
        "\n",
        "print(\"\\n--- Fragmento aleatorio del medio ---\")\n",
        "# Tomamos un punto medio aproximado para ver contenido real de la novela\n",
        "mid_point = len(article_text) // 2\n",
        "print(article_text[mid_point : mid_point + 500])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cP1JdiOIKQWi"
      },
      "source": [
        "## Elegir el tamaño del contexto (ventana de atención)\n",
        "\n",
        "En este caso, al trabajar con un modelo de lenguaje basado en caracteres, consideramos todo el corpus como un documento continuo. El tamaño del contexto define cuántos caracteres hacia atrás mirará el modelo para predecir el siguiente.\n",
        "\n",
        "**La intuición detrás de la ventana:**\n",
        "\n",
        "A diferencia de los modelos basados en palabras (donde un contexto de 10 palabras ya puede capturar una frase completa), en los modelos de caracteres necesitamos una ventana mucho mayor para capturar estructuras gramaticales y semánticas significativas.\n",
        "\n",
        "* **Ventana pequeña (ej. 10):** El modelo solo vería fragmentos sin sentido como \"de la man\".\n",
        "* **Ventana amplia (ej. 100):** El modelo puede ver el equivalente a una o dos oraciones completas, lo que le da suficiente información para aprender patrones más complejos.\n",
        "\n",
        "> **Decisión:** Por esta razón, elegimos `max_context_size = 100` en lugar de un valor menor. Esta elección asegura que el modelo tenga el contexto suficiente para entender la semántica, algo que se perdería con la configuración típica de un modelo de palabras."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Tokenización por Caracteres\n",
        "\n",
        "# Crear el vocabulario único de caracteres\n",
        "# Usamos 'set' para obtener los caracteres únicos y 'sorted' para tener un orden consistente\n",
        "chars_vocab = sorted(list(set(article_text)))\n",
        "vocab_size = len(chars_vocab)\n",
        "\n",
        "# Mapeos de caracteres a índices y viceversa\n",
        "char2idx = {char: idx for idx, char in enumerate(chars_vocab)}\n",
        "idx2char = {idx: char for idx, char in enumerate(chars_vocab)}\n",
        "\n",
        "print(f\"Tamaño del vocabulario de caracteres: {vocab_size}\")\n",
        "print(\"Caracteres encontrados:\", \"\".join(chars_vocab))\n",
        "\n",
        "# Función para tokenizar (Texto -> Índices)\n",
        "def encode_text(text, char_map):\n",
        "    return [char_map[c] for c in text]\n",
        "\n",
        "# okenizamos todo el corpus\n",
        "tokenized_text = encode_text(article_text, char2idx)\n",
        "\n",
        "print(f\"\\nEjemplo de tokenización:\")\n",
        "print(f\"Texto original: '{article_text[:20]}'\")\n",
        "print(f\"Índices: {tokenized_text[:20]}\")"
      ],
      "metadata": {
        "id": "-l6r4O3f9F8O",
        "outputId": "256f4a7e-0a28-409d-91db-79ef3fde3565",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tamaño del vocabulario de caracteres: 62\n",
            "Caracteres encontrados: \n",
            " !\"'(),-.01234567:;?]abcdefghijlmnopqrstuvwxyz¡«»¿àáéíïñóùúü—\n",
            "\n",
            "Ejemplo de tokenización:\n",
            "Texto original: '\n",
            "\n",
            "\n",
            "\n",
            "el ingenioso hid'\n",
            "Índices: [0, 0, 0, 0, 26, 32, 1, 30, 34, 28, 26, 34, 30, 35, 39, 35, 1, 29, 30, 25]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "wumBNwdjJM3j",
        "outputId": "cc6c1b35-d6e9-4150-e389-b6a1ed064d0f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tamaño de contexto definido: 100 caracteres\n"
          ]
        }
      ],
      "source": [
        "max_context_size = 100\n",
        "\n",
        "print(f\"Tamaño de contexto definido: {max_context_size} caracteres\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m5FeTaGvbDbw"
      },
      "outputs": [],
      "source": [
        "# Usaremos las utilidades de procesamiento de textos y secuencias de Keras\n",
        "from tensorflow.keras.utils import pad_sequences # se utilizará para padding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "573Cg5n7VhWw"
      },
      "outputs": [],
      "source": [
        "# en este caso el vocabulario es el conjunto único de caracteres que existe en todo el texto\n",
        "chars_vocab = set(article_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VwTK6xgLJd8q"
      },
      "outputs": [],
      "source": [
        "# la longitud de vocabulario de caracteres es:\n",
        "len(chars_vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2W0AeQjXV1Ou"
      },
      "outputs": [],
      "source": [
        "# Construimos los dicionarios que asignan índices a caracteres y viceversa.\n",
        "# El diccionario `char2idx` servirá como tokenizador.\n",
        "char2idx = {k: v for v,k in enumerate(chars_vocab)}\n",
        "idx2char = {v: k for k,v in char2idx.items()}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2oIUjVU0LB0r"
      },
      "source": [
        "###  Tokenizar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h07G3srdJppo"
      },
      "outputs": [],
      "source": [
        "# tokenizamos el texto completo\n",
        "tokenized_text = [char2idx[ch] for ch in article_text]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PwGVSKOiJ5bj"
      },
      "outputs": [],
      "source": [
        "tokenized_text[:1000]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pfpYcaypKcI9"
      },
      "source": [
        "### Organizando y estructurando el dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WSSmg9jtKP0T"
      },
      "outputs": [],
      "source": [
        "# separaremos el dataset entre entrenamiento y validación.\n",
        "# `p_val` será la proporción del corpus que se reservará para validación\n",
        "# `num_val` es la cantidad de secuencias de tamaño `max_context_size` que se usará en validación\n",
        "p_val = 0.1\n",
        "num_val = int(np.ceil(len(tokenized_text)*p_val/max_context_size))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b7dCpGrdKll0"
      },
      "outputs": [],
      "source": [
        "# separamos la porción de texto utilizada en entrenamiento de la de validación.\n",
        "train_text = tokenized_text[:-num_val*max_context_size]\n",
        "val_text = tokenized_text[-num_val*max_context_size:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NmxQdxl8LRCg"
      },
      "outputs": [],
      "source": [
        "tokenized_sentences_val = [val_text[init*max_context_size:init*(max_context_size+1)] for init in range(num_val)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_gyFT9koLqDm"
      },
      "outputs": [],
      "source": [
        "tokenized_sentences_train = [train_text[init:init+max_context_size] for init in range(len(train_text)-max_context_size+1)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oVNqmmLRodT0"
      },
      "outputs": [],
      "source": [
        "X = np.array(tokenized_sentences_train[:-1])\n",
        "y = np.array(tokenized_sentences_train[1:])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vken7O4ETsAJ"
      },
      "source": [
        "Nótese que estamos estructurando el problema de aprendizaje como *many-to-many*:\n",
        "\n",
        "Entrada: secuencia de tokens [$x_0$, $x_1$, ..., $x_N$]\n",
        "\n",
        "Target: secuencia de tokens [$x_1$, $x_2$, ..., $x_{N+1}$]\n",
        "\n",
        "De manera que la red tiene que aprender que su salida deben ser los tokens desplazados en una posición y un nuevo token predicho (el N+1).\n",
        "\n",
        "La ventaja de estructurar el aprendizaje de esta manera es que para cada token de target se propaga una señal de gradiente por el grafo de cómputo recurrente, que es mejor que estructurar el problema como *many-to-one* en donde sólo una señal de gradiente se propaga."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l3iPTx-UJl6r"
      },
      "source": [
        "En este punto tenemos en la variable `tokenized_sentences` los versos tokenizados. Vamos a quedarnos con un conjunto de validación que utilizaremos para medir la calidad de la generación de secuencias con la métrica de Perplejidad."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KFAyA4zCWE-5"
      },
      "outputs": [],
      "source": [
        "X.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qcKRl70HFTzG"
      },
      "outputs": [],
      "source": [
        "X[0,:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TVpLCKSZFXZO"
      },
      "outputs": [],
      "source": [
        "y[0,:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wOFCR-KqbW1N"
      },
      "outputs": [],
      "source": [
        "vocab_size = len(chars_vocab)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tnnjdAQ5UAEJ"
      },
      "source": [
        "# Definiendo el modelo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wgz7VKwTUbj6"
      },
      "source": [
        "El modelo que se propone como ejemplo consume los índices de los tokens y los transforma en vectores OHE (en este caso no entrenamos una capa de embedding para caracteres). Esa transformación se logra combinando las capas `CategoryEncoding` que transforma a índices a vectores OHE y `TimeDistributed` que aplica la capa a lo largo de la dimensión \"temporal\" de la secuencia."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class RNNModel(nn.Module):\n",
        "    def __init__(self, vocab_size, hidden_size=200):\n",
        "        super().__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.rnn = nn.RNN(\n",
        "            input_size=vocab_size,\n",
        "            hidden_size=hidden_size,\n",
        "            batch_first=True\n",
        "        )\n",
        "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (batch, seq_len, 1) con índices enteros\n",
        "        x = x.squeeze(-1).long()                   # (batch, seq_len)\n",
        "        x = F.one_hot(x, num_classes=self.vocab_size).float()  # (batch, seq_len, vocab_size)\n",
        "\n",
        "        out, _ = self.rnn(x)                      # (batch, seq_len, hidden_size)\n",
        "        out = self.fc(out)                        # (batch, seq_len, vocab_size)\n",
        "        return out                                # logits (sin softmax)\n",
        "\n",
        "\n",
        "model = RNNModel(vocab_size).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.RMSprop(model.parameters(), lr=0.001)\n",
        "\n",
        "print(model)"
      ],
      "metadata": {
        "id": "_i4UGQwRaEwF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GmJWNyxQwfCE"
      },
      "source": [
        "\n",
        "### Definir el modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zUHX3r5JD-MG"
      },
      "outputs": [],
      "source": [
        "class TrainerWithPerplexity:\n",
        "    def __init__(self, model, optimizer, criterion, train_loader, val_loader, patience=5, device=\"cpu\"):\n",
        "        self.model = model.to(device)\n",
        "        self.optimizer = optimizer\n",
        "        self.criterion = criterion\n",
        "        self.train_loader = train_loader\n",
        "        self.val_loader = val_loader\n",
        "        self.patience = patience\n",
        "        self.device = device\n",
        "\n",
        "        self.min_score = float(\"inf\")\n",
        "        self.patience_counter = 0\n",
        "        self.history_ppl = []\n",
        "\n",
        "    def compute_perplexity(self):\n",
        "        self.model.eval()\n",
        "        scores = []\n",
        "        with torch.no_grad():\n",
        "            for xb, yb in self.val_loader:\n",
        "                xb, yb = xb.to(self.device), yb.to(self.device)\n",
        "                logits = self.model(xb)  # (batch, seq_len, vocab_size)\n",
        "\n",
        "                # tomamos la probabilidad del último token predicho\n",
        "                log_probs = F.log_softmax(logits, dim=-1)\n",
        "\n",
        "                target = yb[:, -1]  # último token\n",
        "                probs = log_probs[:, -1, :]\n",
        "\n",
        "                chosen_log_probs = probs[range(len(target)), target]\n",
        "                ppl = torch.exp(-chosen_log_probs.mean()).item()\n",
        "                scores.append(ppl)\n",
        "        return np.mean(scores)\n",
        "\n",
        "    def train(self, num_epochs=20, save_path=\"best_model.pt\"):\n",
        "        for epoch in range(num_epochs):\n",
        "            self.model.train()\n",
        "            total_loss = 0\n",
        "            for xb, yb in self.train_loader:\n",
        "                xb, yb = xb.to(self.device), yb.to(self.device)\n",
        "                self.optimizer.zero_grad()\n",
        "                logits = self.model(xb)\n",
        "\n",
        "                # logits: (batch, seq_len, vocab_size)\n",
        "                # target: (batch, seq_len)\n",
        "                loss = self.criterion(logits.transpose(1, 2), yb)\n",
        "                loss.backward()\n",
        "                self.optimizer.step()\n",
        "                total_loss += loss.item()\n",
        "\n",
        "            avg_loss = total_loss / len(self.train_loader)\n",
        "\n",
        "            # calcular ppl en validación\n",
        "            current_ppl = self.compute_perplexity()\n",
        "            self.history_ppl.append(current_ppl)\n",
        "\n",
        "            print(f\"Epoch {epoch+1}: train loss={avg_loss:.4f}, val ppl={current_ppl:.4f}\")\n",
        "\n",
        "            # early stopping\n",
        "            if current_ppl < self.min_score:\n",
        "                self.min_score = current_ppl\n",
        "                torch.save(self.model.state_dict(), save_path)\n",
        "                print(\"Saved new best model!\")\n",
        "                self.patience_counter = 0\n",
        "            else:\n",
        "                self.patience_counter += 1\n",
        "                if self.patience_counter >= self.patience:\n",
        "                    break\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8HBZIwR0gruA"
      },
      "source": [
        "### Entrenamiento"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# Parámetros\n",
        "batch_size = 256\n",
        "num_epochs = 20\n",
        "patience = 5\n",
        "\n",
        "train_dataset = torch.utils.data.TensorDataset(torch.tensor(X, dtype=torch.long),\n",
        "                                               torch.tensor(y, dtype=torch.long))\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "def prepare_val_data(val_data, max_context_size, vocab_size):\n",
        "    targets, padded = [], []\n",
        "    info = []\n",
        "    count = 0\n",
        "\n",
        "    for seq in val_data:\n",
        "        len_seq = len(seq)\n",
        "        subseq = [seq[:i] for i in range(1, len_seq)]\n",
        "        targets.extend([seq[i] for i in range(1, len_seq)])\n",
        "\n",
        "        if len(subseq) != 0:\n",
        "            arr = np.zeros((len(subseq), max_context_size), dtype=np.int64)\n",
        "            for j, s in enumerate(subseq):\n",
        "                # truncar si es más larga\n",
        "                s = s[-max_context_size:]\n",
        "                arr[j, -len(s):] = s\n",
        "            padded.append(arr)\n",
        "            info.append((count, count + len_seq))\n",
        "            count += len_seq\n",
        "\n",
        "    padded = np.vstack(padded)\n",
        "    return torch.tensor(padded, dtype=torch.long), torch.tensor(targets, dtype=torch.long), info\n",
        "\n",
        "\n",
        "def compute_perplexity(model, val_inputs, val_targets, batch_size=256):\n",
        "    model.eval()\n",
        "    all_log_probs = []\n",
        "    with torch.no_grad():\n",
        "        for i in range(0, len(val_inputs), batch_size):\n",
        "            xb = val_inputs[i:i+batch_size].to(device)\n",
        "            yb = val_targets[i:i+batch_size].to(device)\n",
        "\n",
        "            logits = model(xb.unsqueeze(-1))  # tu modelo espera (batch, seq_len, 1)\n",
        "            log_probs = F.log_softmax(logits[:, -1, :], dim=-1)\n",
        "            chosen = log_probs[range(len(yb)), yb]\n",
        "            all_log_probs.extend(chosen.cpu().numpy())\n",
        "\n",
        "    all_log_probs = np.array(all_log_probs)\n",
        "    ppl = float(np.exp(-all_log_probs.mean()))\n",
        "    return ppl\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.RMSprop(model.parameters(), lr=0.001)\n",
        "\n",
        "history_ppl = []\n",
        "min_score = float(\"inf\")\n",
        "patience_counter = 0\n",
        "\n",
        "# preparar datos de validación\n",
        "val_inputs, val_targets, val_info = prepare_val_data(tokenized_sentences_val,\n",
        "                                                     max_context_size=max_context_size,\n",
        "                                                     vocab_size=vocab_size)\n",
        "\n",
        "# --- training loop ---\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for xb, yb in train_loader:\n",
        "        xb, yb = xb.to(device), yb.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(xb.unsqueeze(-1))   # logits: (batch, seq_len, vocab_size)\n",
        "        loss = criterion(logits.transpose(1, 2), yb)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "    current_ppl = compute_perplexity(model, val_inputs, val_targets, batch_size=batch_size)\n",
        "    history_ppl.append(current_ppl)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs} - loss: {avg_loss:.4f} - val_ppl: {current_ppl:.4f}\")\n",
        "\n",
        "    # early stopping\n",
        "    if current_ppl < min_score:\n",
        "        min_score = current_ppl\n",
        "        torch.save(model.state_dict(), \"best_model.pt\")\n",
        "        print(\"Saved new best model!\")\n",
        "        patience_counter = 0\n",
        "    else:\n",
        "        patience_counter += 1\n",
        "        if patience_counter >= patience:\n",
        "            print(\"Early stopping triggered\")\n",
        "            break\n"
      ],
      "metadata": {
        "id": "hFPA_e2kdWj9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K30JHB3Dv-mx"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Entrenamiento\n",
        "epoch_count = range(1, len(history_ppl) + 1)\n",
        "sns.lineplot(x=epoch_count,  y=history_ppl)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KN6Fg_BsxJe6"
      },
      "source": [
        "\n",
        "### Predicción del próximo caracter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IBvKHFPmzpy2"
      },
      "outputs": [],
      "source": [
        "# Se puede usar gradio para probar el modelo\n",
        "# Gradio es una herramienta muy útil para crear interfaces para ensayar modelos\n",
        "# https://gradio.app/\n",
        "\n",
        "!pip install -q gradio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HNyBykvhzs7-"
      },
      "outputs": [],
      "source": [
        "import gradio as gr\n",
        "\n",
        "def model_response(human_text):\n",
        "    model.eval()  # modo evaluación\n",
        "\n",
        "    # encodeamos\n",
        "    encoded = [char2idx.get(ch, 0) for ch in human_text.lower()]  # si el char no está, 0\n",
        "    if len(encoded) > max_context_size:\n",
        "        encoded = encoded[-max_context_size:]  # truncar\n",
        "    else:\n",
        "        encoded = [0]*(max_context_size - len(encoded)) + encoded  # pad izquierda\n",
        "\n",
        "    # tensor al device correcto\n",
        "    x = torch.tensor(encoded, dtype=torch.long).unsqueeze(0).unsqueeze(-1).to(device)\n",
        "    # shape: (1, seq_len=max_context_size, 1)\n",
        "\n",
        "    # forward\n",
        "    with torch.no_grad():\n",
        "        logits = model(x)              # (1, seq_len, vocab_size)\n",
        "        probs = F.softmax(logits[0, -1, :], dim=-1)  # último timestep\n",
        "\n",
        "    y_hat = torch.argmax(probs).item()\n",
        "    out_word = idx2char[y_hat]\n",
        "\n",
        "    return human_text + out_word\n",
        "\n",
        "iface = gr.Interface(\n",
        "    fn=model_response,\n",
        "    inputs=[\"textbox\"],\n",
        "    outputs=\"text\")\n",
        "\n",
        "iface.launch(debug=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mCeMWWupxN1-"
      },
      "source": [
        "### Generación de secuencias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bwbS_pfhxvB3"
      },
      "outputs": [],
      "source": [
        "def generate_seq(model, seed_text, max_length, n_words):\n",
        "    model.eval()\n",
        "    device = next(model.parameters()).device  # detecta si está en cpu o cuda\n",
        "    output_text = seed_text\n",
        "\n",
        "    for _ in range(n_words):\n",
        "        encoded = [char2idx.get(ch, 0) for ch in output_text.lower()]  # si no está → 0\n",
        "\n",
        "        # truncar o padear a max_length\n",
        "        if len(encoded) > max_length:\n",
        "            encoded = encoded[-max_length:]\n",
        "        else:\n",
        "            encoded = [0] * (max_length - len(encoded)) + encoded\n",
        "\n",
        "        # convertir a tensor (batch=1, seq_len=max_length, 1)\n",
        "        x = torch.tensor(encoded, dtype=torch.long).unsqueeze(0).unsqueeze(-1).to(device)\n",
        "\n",
        "        # --- Forward ---\n",
        "        with torch.no_grad():\n",
        "            logits = model(x)  # (1, seq_len, vocab_size)\n",
        "            probs = F.softmax(logits[0, -1, :], dim=-1)\n",
        "            y_hat = torch.argmax(probs).item()\n",
        "\n",
        "        # convertir a caracter\n",
        "        out_char = idx2char[y_hat]\n",
        "        output_text += out_char\n",
        "\n",
        "    return output_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JoFqRC5pxzqS"
      },
      "outputs": [],
      "source": [
        "input_text='habia una vez'\n",
        "\n",
        "generate_seq(model, input_text, max_length=max_context_size, n_words=30)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "drJ6xn5qW1Hl"
      },
      "source": [
        "###  Beam search y muestreo aleatorio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_vovn9XZW1Hl"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "def encode(text, max_length=max_context_size, device=None):\n",
        "    # convertir cada caracter a índice (si no está en vocabulario, usar 0)\n",
        "    encoded = [char2idx.get(ch, 0) for ch in text.lower()]\n",
        "\n",
        "    # truncar o padear\n",
        "    if len(encoded) > max_length:\n",
        "        encoded = encoded[-max_length:]\n",
        "    else:\n",
        "        encoded = [0] * (max_length - len(encoded)) + encoded\n",
        "\n",
        "    # convertir a tensor (batch=1, seq_len, 1)\n",
        "    tensor = torch.tensor(encoded, dtype=torch.long).unsqueeze(0).unsqueeze(-1)\n",
        "\n",
        "    if device is not None:\n",
        "        tensor = tensor.to(device)\n",
        "    return tensor\n",
        "\n",
        "\n",
        "def decode(seq):\n",
        "    if torch.is_tensor(seq):\n",
        "        seq = seq.cpu().numpy().tolist()\n",
        "    return ''.join([idx2char[ch] for ch in seq])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I_lZiQwkW1Hl"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from scipy.special import softmax\n",
        "\n",
        "# función que selecciona candidatos para el beam search\n",
        "def select_candidates(pred, num_beams, vocab_size, history_probs, history_tokens, temp, mode):\n",
        "    pred_large = []\n",
        "\n",
        "    for idx, pp in enumerate(pred):\n",
        "        # sumamos los log probs acumulados\n",
        "        pred_large.extend(np.log(pp + 1E-10) + history_probs[idx])\n",
        "\n",
        "    pred_large = np.array(pred_large)\n",
        "\n",
        "    # criterio de selección\n",
        "    if mode == 'det':\n",
        "        idx_select = np.argsort(pred_large)[::-1][:num_beams]  # beam search determinista\n",
        "    elif mode == 'sto':\n",
        "        idx_select = np.random.choice(\n",
        "            np.arange(pred_large.shape[0]),\n",
        "            num_beams,\n",
        "            p=softmax(pred_large / temp)\n",
        "        )\n",
        "    else:\n",
        "        raise ValueError(f\"Wrong selection mode: {mode}. Use 'det' or 'sto'.\")\n",
        "\n",
        "    new_history_tokens = np.concatenate(\n",
        "        (np.array(history_tokens)[idx_select // vocab_size],\n",
        "         np.array([idx_select % vocab_size]).T),\n",
        "        axis=1\n",
        "    )\n",
        "\n",
        "    return pred_large[idx_select.astype(int)], new_history_tokens.astype(int)\n",
        "\n",
        "\n",
        "def beam_search(model, num_beams, num_words, input_text, max_length,temp=1.0, mode='det'):\n",
        "    model.eval()\n",
        "    device = next(model.parameters()).device\n",
        "\n",
        "    encoded = encode(input_text, max_length=max_length, device=device)  # (1, seq_len, 1)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        logits = model(encoded)  # (1, seq_len, vocab_size)\n",
        "        probs = F.softmax(logits[0, -1, :], dim=-1).cpu().numpy()\n",
        "\n",
        "    vocab_size = probs.shape[0]\n",
        "\n",
        "    history_probs = [0] * num_beams\n",
        "    history_tokens = [encoded.squeeze(-1).cpu().numpy()[0]] * num_beams  # shape: (seq_len,)\n",
        "\n",
        "    # seleccionar primeros candidatos\n",
        "    history_probs, history_tokens = select_candidates([probs],\n",
        "                                                      num_beams,\n",
        "                                                      vocab_size,\n",
        "                                                      history_probs,\n",
        "                                                      history_tokens,\n",
        "                                                      temp,\n",
        "                                                      mode)\n",
        "\n",
        "    #loop beam search\n",
        "    for i in range(num_words - 1):\n",
        "        preds = []\n",
        "\n",
        "        for hist in history_tokens:\n",
        "            # mantener contexto de tamaño max_length\n",
        "            input_update = hist[-max_length:]\n",
        "            x = torch.tensor(input_update, dtype=torch.long).unsqueeze(0).unsqueeze(-1).to(device)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                logits = model(x)\n",
        "                y_hat = F.softmax(logits[0, -1, :], dim=-1).cpu().numpy()\n",
        "\n",
        "            preds.append(y_hat)\n",
        "\n",
        "        history_probs, history_tokens = select_candidates(preds,\n",
        "                                                          num_beams,\n",
        "                                                          vocab_size,\n",
        "                                                          history_probs,\n",
        "                                                          history_tokens,\n",
        "                                                          temp,\n",
        "                                                          mode)\n",
        "\n",
        "    # devolver secuencias generadas (últimos tokens relevantes)\n",
        "    return history_tokens[:, -(len(input_text) + num_words):]\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max_context_size"
      ],
      "metadata": {
        "id": "0s5n5TqDp74_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GeLqAoOYW1Hm"
      },
      "outputs": [],
      "source": [
        "# predicción con beam search\n",
        "salidas = beam_search(model,num_beams=10,num_words=200,input_text=\"habia una vez\",max_length=max_context_size,temp=1,mode=\"sto\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P8HQoLhw-NYg"
      },
      "outputs": [],
      "source": [
        "salidas[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2S3_I3S1W1Hm"
      },
      "outputs": [],
      "source": [
        "# veamos las salidas\n",
        "decode(salidas[0])"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}